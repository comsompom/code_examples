"""
CSV Files Upload to BigQuery Script

This script uploads all CSV files generated by populate_stock_data.py to BigQuery.
It processes multiple CSV files and uploads them to the BigQuery table.
"""

import os
import glob
import pandas as pd
from google.cloud import bigquery
from google.cloud.exceptions import NotFound
import logging

# Set environment variables directly
os.environ['GOOGLE_CLOUD_PROJECT'] = 'bq-example-468708'
os.environ['BIGQUERY_DATASET_ID'] = 'stock_prices_data'

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CSVDataUploader:
    def __init__(self):
        """Initialize the CSV data uploader."""
        self.project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
        self.dataset_id = os.getenv('BIGQUERY_DATASET_ID', 'stock_prices_data')
        self.table_id = "stock_prices"
        self.table_ref = f"{self.project_id}.{self.dataset_id}.{self.table_id}"
        
        # Initialize BigQuery client
        self.client = bigquery.Client(project=self.project_id)
        
    def verify_table_exists(self):
        """Verify that the BigQuery table exists."""
        try:
            self.client.get_table(self.table_ref)
            logger.info(f"Table {self.table_ref} exists")
            return True
        except NotFound:
            logger.error(f"Table {self.table_ref} does not exist")
            logger.error("Please run create_bigquery_timeseries_table.py first")
            return False
    
    def find_csv_files(self):
        """Find all CSV files in the current directory."""
        csv_pattern = "stock_prices_data_part_*.csv"
        csv_files = glob.glob(csv_pattern)
        csv_files.sort()  # Sort to ensure consistent order
        
        logger.info(f"Found {len(csv_files)} CSV files: {csv_files}")
        return csv_files
    
    def read_csv_file(self, file_path):
        """Read a CSV file and return a DataFrame."""
        try:
            logger.info(f"Reading CSV file: {file_path}")
            df = pd.read_csv(file_path)
            logger.info(f"Loaded {len(df)} records from {file_path}")
            return df
        except Exception as e:
            logger.error(f"Error reading {file_path}: {e}")
            return None
    
    def clean_dataframe(self, df):
        """Clean and validate the DataFrame."""
        if df is None or df.empty:
            return df
        
        logger.info(f"Cleaning DataFrame with {len(df)} records...")
        
        # Convert timestamp columns to datetime
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
        if 'created_at' in df.columns:
            df['created_at'] = pd.to_datetime(df['created_at'])
        if 'updated_at' in df.columns:
            df['updated_at'] = pd.to_datetime(df['updated_at'])
        
        # Filter to a reasonable date range to avoid partition limit issues
        # BigQuery free tier allows max 4000 partitions, so we'll limit to ~2 years of data
        start_date = pd.Timestamp('2024-01-01')
        end_date = pd.Timestamp('2025-12-31')
        
        initial_count = len(df)
        df = df[(df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)]
        if len(df) < initial_count:
            logger.info(f"Filtered data from {initial_count} to {len(df)} records (date range: {start_date.date()} to {end_date.date()})")
        
        # Ensure numeric fields are properly formatted
        numeric_fields = ['open', 'high', 'low', 'close', 'volume', 'adjusted_close', 'dividend_amount', 'split_coefficient']
        for field in numeric_fields:
            if field in df.columns:
                df[field] = pd.to_numeric(df[field], errors='coerce')
        
        # Remove any rows with NaN values in required fields
        required_fields = ['timestamp', 'symbol', 'exchange', 'open', 'high', 'low', 'close', 'volume', 'interval']
        initial_count = len(df)
        df = df.dropna(subset=required_fields)
        if len(df) < initial_count:
            logger.warning(f"Removed {initial_count - len(df)} rows with missing required fields")
        
        logger.info(f"Cleaned DataFrame: {len(df)} records")
        return df
    
    def upload_dataframe_to_bigquery(self, df, file_name):
        """Upload a DataFrame to BigQuery."""
        try:
            if df is None or df.empty:
                logger.warning(f"Skipping {file_name} - no valid data")
                return False
            
            logger.info(f"Uploading {len(df)} records from {file_name}")
            logger.info(f"DataFrame shape: {df.shape}")
            logger.info(f"DataFrame columns: {list(df.columns)}")
            
            # Create job configuration
            job_config = bigquery.LoadJobConfig(
                write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
                autodetect=False,  # Use existing schema
                source_format=bigquery.SourceFormat.CSV,
                max_bad_records=0  # Don't allow bad records
            )
            
            # Upload to BigQuery
            job = self.client.load_table_from_dataframe(
                df, 
                self.table_ref,
                job_config=job_config
            )
            
            # Wait for the job to complete
            logger.info(f"Waiting for {file_name} upload to complete...")
            job.result()
            
            # Check job status
            if job.errors:
                logger.error(f"Job completed with errors for {file_name}: {job.errors}")
                return False
            
            logger.info(f"Successfully uploaded {len(df)} records from {file_name}")
            logger.info(f"Job ID: {job.job_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error uploading {file_name}: {e}")
            return False
    
    def verify_upload(self):
        """Verify the upload by counting records in the table."""
        try:
            query = f"SELECT COUNT(*) as total_records FROM `{self.project_id}.{self.dataset_id}.{self.table_id}`"
            logger.info(f"Running verification query: {query}")
            query_job = self.client.query(query)
            results = query_job.result()
            
            for row in results:
                total_records = row.total_records
                logger.info(f"Total records in table: {total_records:,}")
                return total_records
                
        except Exception as e:
            logger.error(f"Error verifying upload: {e}")
            return 0
    
    def run_upload(self):
        """Run the complete CSV upload process."""
        logger.info("Starting CSV files upload to BigQuery...")
        
        # Verify table exists
        if not self.verify_table_exists():
            return False
        
        # Find CSV files
        csv_files = self.find_csv_files()
        if not csv_files:
            logger.error("No CSV files found. Please run populate_stock_data.py first to generate CSV files.")
            return False
        
        # Get initial record count
        initial_records = self.verify_upload()
        logger.info(f"Initial records in table: {initial_records:,}")
        
        # Upload each CSV file
        total_uploaded = 0
        successful_uploads = 0
        
        for csv_file in csv_files:
            logger.info(f"\nProcessing {csv_file}...")
            
            # Read and clean the CSV file
            df = self.read_csv_file(csv_file)
            df = self.clean_dataframe(df)
            
            # Upload to BigQuery
            if self.upload_dataframe_to_bigquery(df, csv_file):
                total_uploaded += len(df) if df is not None else 0
                successful_uploads += 1
            else:
                logger.error(f"Failed to upload {csv_file}")
        
        # Final verification
        logger.info("\nWaiting 10 seconds for BigQuery to process all data...")
        import time
        time.sleep(10)
        
        final_records = self.verify_upload()
        records_added = final_records - initial_records
        
        logger.info("Upload process completed!")
        logger.info(f"Files processed: {len(csv_files)}")
        logger.info(f"Successful uploads: {successful_uploads}")
        logger.info(f"Records uploaded: {total_uploaded:,}")
        logger.info(f"Records added to table: {records_added:,}")
        logger.info(f"Final table count: {final_records:,}")
        
        return successful_uploads == len(csv_files)


def main():
    """Main function to run the CSV upload process."""
    try:
        uploader = CSVDataUploader()
        success = uploader.run_upload()
        
        if success:
            print("\n" + "="*60)
            print("CSV FILES UPLOAD TO BIGQUERY COMPLETED")
            print("="*60)
            print("Successfully uploaded all CSV files")
            print("Data is now available in BigQuery")
            print(f"Table: {uploader.table_ref}")
            print("\nYou can now run anomaly_detection.py to analyze the data")
        else:
            print("Upload process failed")
            
    except Exception as e:
        logger.error(f"Upload process failed: {e}")
        raise

if __name__ == "__main__":
    main()
